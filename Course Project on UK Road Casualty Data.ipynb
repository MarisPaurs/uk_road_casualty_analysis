{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8dd44c-ba66-475c-881e-4c76fbdb24fe",
   "metadata": {},
   "source": [
    "## UK Road Casualty Analysis with LightGBM\n",
    "\n",
    "The Road Casualty Statistics datasets was obtained from:\n",
    "https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-casualty-2023.csv\n",
    "https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-vehicle-2023.csv\n",
    "https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-collision-2023.csv\n",
    "\n",
    "The variables and values (coding) are described here:\n",
    "https://doc.ukdataservice.ac.uk/doc/5244/mrdoc/pdf/5244userguide.pdf\n",
    "\n",
    "\n",
    "### Abstract\n",
    "\n",
    "The Light-GBM classifier was used on UK vehicle collision dataset from 2023 to model the severity of casualty parameter. Decent baseline performance metrics were achieved with True Positive vs False Positive AUC ~ 0.725 and Precision-Recall AUC ~ 0.417.\n",
    " \n",
    "\n",
    "### Data and objectives\n",
    "\n",
    "The dataset is made public by the UK government.\n",
    "The data is provided in 3 individual sets:\n",
    "- casualty dataset\n",
    "- vehicle dataset\n",
    "- collision dataset\n",
    "\n",
    "All three subsets are linked by accident_reference key.\n",
    "I want to try and see if I can model the severity of injuries sustained by crash victims using features accross the 3 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490c4b4-e9b4-48ff-9157-153aae45107e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5236262-5cae-452e-a121-2d8cd50bce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# loading the three datasets for 2023\n",
    "casualties = pd.read_csv(\"dft-road-casualty-statistics-casualty-2023.csv\", low_memory = False)\n",
    "collisions = pd.read_csv(\"dft-road-casualty-statistics-collision-2023.csv\", low_memory = False)\n",
    "vehicles = pd.read_csv(\"dft-road-casualty-statistics-vehicle-2023.csv\", low_memory = False)\n",
    "\n",
    "# transferring to working copies\n",
    "cas = casualties.copy()\n",
    "acc = collisions.copy()\n",
    "veh = vehicles.copy()\n",
    "\n",
    "# we are looking at over a 100k rows and 10s of features\n",
    "print(cas.shape)\n",
    "print(acc.shape)\n",
    "print(veh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415511aa-fe27-419a-a15c-16cabad546f6",
   "metadata": {},
   "source": [
    "#### Let's do some initial Exploratory Data Analysis\n",
    "Casualty severity maps as 1, 2, 3 = Fatal, Serious, Slight. We can see that injuries are predominantly Slight and there is also a significant proportion that are classified as Serious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fdb30a-eed7-4a56-8f21-9831e9f503cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at casualty severity, speed limit and age of casualty variable distributions to get a feel for it\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.countplot(x='casualty_severity', data=cas)\n",
    "plt.title('Casualty Severity Distribution')\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(acc['speed_limit'].dropna(), bins=10, kde=False)\n",
    "plt.title('Speed Limit Distribution')\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(cas['age_of_casualty'].dropna(), bins=10, kde=False)\n",
    "plt.title('Age of Casualty Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611365e-8875-4634-8a4a-7f46541441ab",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcb2eb-0212-47c2-8c23-ac3099ac7846",
   "metadata": {},
   "source": [
    "#### CASUALTY dataset cleaning - let's remove some columns and create new columns if necessary\n",
    "We'll use:\n",
    "accident_reference,\n",
    "vehicle_reference,\n",
    "casualty_class_lbl,\n",
    "sex_of_casualty_lbl,\n",
    "age_band,\n",
    "target which will be binary - 0 for Slight and 1 for Serious or Fatal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057fbeb-8664-4639-8b9c-4305ac2c3d06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's map casualty_class so they are easier to understand -----------------------------------------------------\n",
    "cas_map = {\n",
    "     1: \"driver\",\n",
    "     2: \"passenger\",\n",
    "     3: \"pedestrian\"\n",
    "}\n",
    "\n",
    "# creating a casualty class label column\n",
    "cas[\"casualty_class_lbl\"] = (\n",
    "    cas[\"casualty_class\"]\n",
    "          .map(cas_map)\n",
    "          .fillna(\"other\")\n",
    ")\n",
    "# categorical\n",
    "cas[\"casualty_class_lbl\"] = cas[\"casualty_class_lbl\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# let's label sex_of_casualty so it's clearer and create a new sex_of_casualty_lbl column ------------------------------------------\n",
    "sex_map = {\n",
    "    1: \"male\",\n",
    "    2: \"female\"\n",
    "}\n",
    "\n",
    "cas[\"sex_of_casualty_lbl\"] = (\n",
    "    cas[\"sex_of_casualty\"]\n",
    "          .map(sex_map)\n",
    "          .fillna(\"unknown\")   # handles any unexpected code\n",
    ")\n",
    "\n",
    "# categorical\n",
    "cas[\"sex_of_casualty_lbl\"] = cas[\"sex_of_casualty_lbl\"].astype(\"category\")\n",
    "#cas[\"age_of_casualty\"].value_counts()\n",
    "#cas[\"sex_of_casualty_lbl\"].value_counts()\n",
    "\n",
    "\n",
    "# let's put age_of_casualty into age_band bins -------------------------------------------------------------------------------------\n",
    "age_bins = [0, 17, 30, 50, 70, 99]\n",
    "age_labels = [\"0-17\", \"18-30\", \"31-50\", \"51-70\", \"71-99\"]\n",
    "\n",
    "cas[\"age_band\"] = pd.cut(\n",
    "    cas[\"age_of_casualty\"].replace(-1, np.nan),\n",
    "    bins=age_bins,\n",
    "    labels=age_labels,\n",
    "    right=True\n",
    ").astype(object).fillna(\"unknown\") # taking care on -1 (NaN) by marking them as unknown\n",
    "\n",
    "# we'll make them categorical in a way where order matters\n",
    "cas[\"age_band\"] = pd.Categorical(\n",
    "    cas[\"age_band\"],\n",
    "    categories=[\"0-17\", \"18-30\", \"31-50\", \"51-70\", \"71-99\", \"unknown\"],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# let's get casualty_severity into two binary bins -------------------------------------------------------------------------------------------------\n",
    "\n",
    "severity_map = {\n",
    "    1: \"Fatal\",\n",
    "    2: \"Serious\",\n",
    "    3: \"Slight\"\n",
    "}\n",
    "\n",
    "cas[\"casualty_severity_lbl\"] = (\n",
    "    cas[\"casualty_severity\"]\n",
    "        .map(severity_map)\n",
    "        .fillna(\"Unknown\")  # handling unexpected codes like -1\n",
    ")\n",
    "\n",
    "# Making casualty severity ordered\n",
    "cas[\"casualty_severity_lbl\"] = pd.Categorical(\n",
    "    cas[\"casualty_severity_lbl\"],\n",
    "    categories=[\"Slight\", \"Serious\", \"Fatal\"],  # ← ascending order\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Create binary target: 1 = Serious or Fatal, 0 = Slight\n",
    "cas[\"target\"] = cas[\"casualty_severity_lbl\"].isin([\"Serious\", \"Fatal\"]).astype(int)\n",
    "\n",
    "# ok now let's only keep what we need -----------------------------------------------------------------------------------------------\n",
    "cas_simple = cas[[\n",
    "    \"accident_reference\",\n",
    "    \"vehicle_reference\",\n",
    "    \"casualty_class_lbl\",\n",
    "    \"sex_of_casualty_lbl\",\n",
    "    \"age_band\",\n",
    "    \"target\"\n",
    "]].copy()\n",
    "\n",
    "# converting accident reference to category\n",
    "cas_simple[\"accident_reference\"] = cas_simple[\"accident_reference\"].astype(\"category\")\n",
    "\n",
    "#cas_simple.info()\n",
    "#cas_simple.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5790a5-75dd-4a1a-96ba-96a08e9e9a74",
   "metadata": {},
   "source": [
    "#### COLLISION dataset cleaning - let's remove some columns and create new columns if necessary\n",
    "We'll use:\n",
    "accident_reference,\n",
    "number_of_vehicles,\n",
    "day_of_week,\n",
    "road_type_grp,\n",
    "light_conditions_grp,\n",
    "road_surface_conditions_grp,\n",
    "area_type_lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc4261-e294-45c4-8cae-cfb796f3e289",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# day_of_week to ordered category -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "day_map = {\n",
    "    1: \"Sunday\",\n",
    "    2: \"Monday\",\n",
    "    3: \"Tuesday\",\n",
    "    4: \"Wednesday\",\n",
    "    5: \"Thursday\",\n",
    "    6: \"Friday\",\n",
    "    7: \"Saturday\"\n",
    "}\n",
    "\n",
    "acc[\"day_of_week_lbl\"] = acc[\"day_of_week\"].map(day_map)\n",
    "\n",
    "acc[\"day_of_week_lbl\"] = pd.Categorical(\n",
    "    acc[\"day_of_week_lbl\"],\n",
    "    categories=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "\n",
    "# road_type to category and bins -----------------------------------------------------------------------------------------------------\n",
    "# Define mapping using a dictionary\n",
    "road_type_map = {\n",
    "    1: \"roundabout\",\n",
    "    2: \"unidirection\",\n",
    "    3: \"unidirection\",\n",
    "    4: \"unidirection\",\n",
    "    5: \"bidirection\",\n",
    "    6: \"bidirection\",\n",
    "    7: \"bidirection\",\n",
    "    8: \"bidirection\",\n",
    "    9: \"bidirection\"\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "acc[\"road_type_grp\"] = acc[\"road_type\"].map(road_type_map).fillna(\"unknown\")\n",
    "\n",
    "# Categorical\n",
    "acc[\"road_type_grp\"] = acc[\"road_type_grp\"].astype(\"category\")\n",
    "\n",
    "# Light conditions -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "light_map = {\n",
    "    1: \"daylight\",\n",
    "    2: \"daylight\",\n",
    "    3: \"daylight\",\n",
    "    4: \"darkness\",\n",
    "    5: \"darkness\",\n",
    "    6: \"darkness\",\n",
    "    7: \"darkness\"\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "acc[\"light_conditions_grp\"] = acc[\"light_conditions\"].map(light_map).fillna(\"unknown\")\n",
    "\n",
    "# Categorical\n",
    "acc[\"light_conditions_grp\"] = acc[\"light_conditions_grp\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# Road surface conditions -------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "surface_map = {\n",
    "    1: \"dry\",\n",
    "    2: \"wet\",\n",
    "    3: \"snow\",\n",
    "    4: \"frost\",\n",
    "    5: \"flood\",\n",
    "    -1: \"unknown\",\n",
    "    9: \"unknown\"\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "acc[\"road_surface_conditions_grp\"] = acc[\"road_surface_conditions\"].map(surface_map).fillna(\"unknown\")\n",
    "\n",
    "# Convert to ordered categorical\n",
    "acc[\"road_surface_conditions_grp\"] = pd.Categorical(\n",
    "    acc[\"road_surface_conditions_grp\"],\n",
    "    categories=[\"dry\", \"wet\", \"frost\", \"snow\", \"flood\", \"unknown\"], # we will drop 'unknown' after merging the datasets\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Urban or rural -----------------------------------------------------------------------------------------------------------------------------------\n",
    "area_map = {\n",
    "    1: \"urban\",\n",
    "    2: \"rural\",\n",
    "    -1: \"unknown\",\n",
    "    3: \"unknown\"\n",
    "}\n",
    "\n",
    "acc[\"area_type_lbl\"] = acc[\"urban_or_rural_area\"].map(area_map).fillna(\"unknown\")\n",
    "\n",
    "# to categorical\n",
    "acc[\"area_type_lbl\"] = acc[\"area_type_lbl\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# keeping only what we need ------------------------------------------------------------------------------------------------------------------------\n",
    "acc_simple = acc[[\n",
    "    \"accident_reference\",\n",
    "    \"number_of_vehicles\",\n",
    "    \"day_of_week_lbl\",\n",
    "    \"road_type_grp\",\n",
    "    \"light_conditions_grp\",\n",
    "    \"road_surface_conditions_grp\",\n",
    "    \"area_type_lbl\"\n",
    "]].copy()\n",
    "\n",
    "# converting accident reference to category\n",
    "acc_simple[\"accident_reference\"] = acc_simple[\"accident_reference\"].astype(\"category\")\n",
    "\n",
    "#acc_simple.info()\n",
    "#acc_simple.head()\n",
    "#acc[\"urban_or_rural_area\"].unique()\n",
    "#acc[\"urban_or_rural_area\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d178186-de65-479c-a8df-72c5a97db7f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### VEHICLE dataset cleaning - let's remove some columns and create new columns if necessary\n",
    "We'll use:\n",
    "accident_reference,\n",
    "vehicle_reference,\n",
    "vehicle_type_grp,\n",
    "sex_of_driver_lbl,\n",
    "age_band_driver,\n",
    "skid_overturn_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960d022-5c21-4212-9468-e7ffb2705615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combining some vehicle types for simplicity---------------------------------------------------------------------\n",
    "veh_map = {\n",
    "     1 : \"bicycle\",\n",
    "     2 : \"motorcycle\",  3 : \"motorcycle\",  4 : \"motorcycle\",  5 : \"motorcycle\",\n",
    "     8 : \"car\",         9 : \"car\",\n",
    "    10 : \"bus\",        11 : \"bus\",\n",
    "    17 : \"agricultural\",\n",
    "    18 : \"tram\",\n",
    "    19 : \"goods\",      20 : \"goods\",      21 : \"goods\"\n",
    "}\n",
    "\n",
    "# Creating vehicle type groups\n",
    "veh[\"vehicle_type_grp\"] = veh[\"vehicle_type\"].map(veh_map).fillna(\"other\")\n",
    "\n",
    "veh[\"vehicle_type_grp\"] = veh[\"vehicle_type_grp\"].astype(\"category\")\n",
    "\n",
    "veh[\"vehicle_type_grp\"].value_counts()\n",
    "#print(veh[\"vehicle_type_grp\"].cat.categories)\n",
    "\n",
    "\n",
    "# sex of driver ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "sex_map = {\n",
    "    1: \"male\",\n",
    "    2: \"female\",\n",
    "    3: \"unknown\"\n",
    "}\n",
    "\n",
    "veh[\"sex_of_driver_lbl\"] = veh[\"sex_of_driver\"].map(sex_map).fillna(\"unknown\")\n",
    "veh[\"sex_of_driver_lbl\"] = veh[\"sex_of_driver_lbl\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# Age of driver ------------------------------------------------------------------------------------------------------------------------\n",
    "# let's identify -1s first\n",
    "veh[\"age_of_driver_clean\"] = veh[\"age_of_driver\"].replace(-1, np.nan)\n",
    "\n",
    "# now let's bin them\n",
    "age_bins = [0, 17, 30, 50, 70, float('inf')]\n",
    "age_labels = [\"0-17\", \"18-30\", \"31-50\", \"51-70\", \"70+\"]\n",
    "\n",
    "veh[\"age_band_driver\"] = pd.cut(\n",
    "    veh[\"age_of_driver_clean\"],\n",
    "    bins=age_bins,\n",
    "    labels=age_labels,\n",
    "    right=True\n",
    ").astype(object).fillna(\"unknown\")\n",
    "\n",
    "# let's make it categorical and ordered just in case\n",
    "veh[\"age_band_driver\"] = pd.Categorical(\n",
    "    veh[\"age_band_driver\"],\n",
    "    categories=[\"0-17\", \"18-30\", \"31-50\", \"51-70\", \"70+\", \"unknown\"],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "\n",
    "# Skidding and overturning ---------------------------------------------------------------------------------------------------------------------\n",
    "# if skidded and not overturned then class as skidded, if skidded and overturned then class as overturned\n",
    "\n",
    "s_o_map = {\n",
    "    0: \"no\",\n",
    "    1: \"skidded\",\n",
    "    2: \"overturned\",\n",
    "    3: \"skidded\",\n",
    "    4: \"overturned\",\n",
    "    5: \"overturned\",\n",
    "    -1: \"unknown\",\n",
    "    9: \"unknown\"\n",
    "}\n",
    "\n",
    "veh[\"skid_overturn_grp\"] = veh[\"skidding_and_overturning\"].map(s_o_map).fillna(\"unknown\")\n",
    "\n",
    "veh[\"skid_overturn_grp\"] = veh[\"skid_overturn_grp\"].astype(\"category\")\n",
    "\n",
    "#veh[\"skidding_and_overturning\"].unique()\n",
    "#veh[\"skidding_and_overturning\"].value_counts()\n",
    "#veh[\"skid_overturn_grp\"].value_counts()\n",
    "\n",
    "# keeping only what we need\n",
    "veh_simple = veh[[\n",
    "    \"accident_reference\",\n",
    "    \"vehicle_reference\",\n",
    "    \"vehicle_type_grp\",\n",
    "    \"sex_of_driver_lbl\",\n",
    "    \"age_band_driver\",\n",
    "    \"skid_overturn_grp\"\n",
    "]].copy()\n",
    "\n",
    "# converting accident reference to category\n",
    "veh_simple[\"accident_reference\"] = veh_simple[\"accident_reference\"].astype(\"category\")\n",
    "\n",
    "#veh_simple.info()\n",
    "#veh_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edf69f-460f-483b-a821-37d78a8ea1eb",
   "metadata": {},
   "source": [
    "### Merging the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9d6ab-5b43-4aa8-8441-6046fb35e884",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# first casualties and vehicles\n",
    "cas_veh = pd.merge(\n",
    "    cas_simple,\n",
    "    veh_simple,\n",
    "    on=[\"accident_reference\", \"vehicle_reference\"],\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"  # so that each casualty links to one vehicle\n",
    ")\n",
    "\n",
    "# and now cas_veh with the collision dataset\n",
    "full_df = pd.merge(\n",
    "    cas_veh,\n",
    "    acc_simple,\n",
    "    on=\"accident_reference\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"  # so that each casualty links to one accident\n",
    ")\n",
    "\n",
    "# let's drop rows with unknown/missing surface conditions\n",
    "full_df = full_df[full_df[\"road_surface_conditions_grp\"] != \"unknown\"]\n",
    "\n",
    "# we will also drop unknowns from urban or rural area type\n",
    "full_df = full_df[full_df[\"area_type_lbl\"] != \"unknown\"]\n",
    "\n",
    "\n",
    "#print(full_df.shape)\n",
    "#full_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2c55a-db8f-4af3-9dbc-64a4bf396aef",
   "metadata": {},
   "source": [
    "### More EDA. \"target\" is 0 for Slight casualty, 1 for Serious or Fatal casualty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421a67c-26f3-4e47-93f4-8b6fd6257bb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eda_df = full_df.sample(20000, random_state=5)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(x='target', hue='casualty_class_lbl', data=eda_df)\n",
    "plt.title('Target by Casualty Class')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(x='target', hue='vehicle_type_grp', data=eda_df)\n",
    "plt.title('Target by Vehicle Type')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.countplot(x='target', hue='sex_of_casualty_lbl', data=eda_df)\n",
    "plt.title('Target by Sex of Casualty')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.countplot(x='target', hue='area_type_lbl', data=eda_df)\n",
    "plt.title('Target by Urban/Rural Area')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='target', y='number_of_vehicles', data=eda_df)\n",
    "plt.title('Number of Vehicles by Target')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='age_band_driver', hue='target', data=eda_df)\n",
    "plt.title('Driver Age Band by Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f6772-3662-48b3-ae0f-564ed6b2b0fc",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811a280-598c-49aa-a820-deee33bb3346",
   "metadata": {},
   "source": [
    "#### Ok, now let's split the data and start modelling using a Gradient Boosted Decision Tree (GBDT) classifier as it deals relatively well with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c4157-c573-4738-9588-48c475c0e448",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dropping what we don't need as predictors\n",
    "X  = full_df.drop(columns=[\"target\", \"accident_reference\", \"vehicle_reference\"])\n",
    "y = full_df[\"target\"]\n",
    "\n",
    "# splitting into 80/20 train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y,     # preserving class balance\n",
    "    random_state=5 # reproducibility\n",
    ")\n",
    "\n",
    "#print(\"Train shape:\", X_train.shape)\n",
    "#print(\"Test shape:\", X_test.shape)\n",
    "#print(\"Target distribution in train:\")\n",
    "#print(y_train.value_counts(normalize=True))\n",
    "#print(\"Target distribution in test:\")\n",
    "#print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# splits look very similarly balanced so should be good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6280f-5154-4d50-91a5-a2749aff2b4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# take categorical columns\n",
    "categorical_cols = X.select_dtypes(include=\"category\").columns.tolist()\n",
    "#print(\"Categorical features:\", categorical_cols)\n",
    "\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    class_weight=\"balanced\", # this addresses the class imbalance (0.21/0.79) in the target variable\n",
    "    learning_rate=0.1, # good default\n",
    "    num_leaves=30, # I tried different # leaves, 30 seems to work ok\n",
    "    n_estimators=1000,\n",
    "    random_state=5\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    categorical_feature=categorical_cols,\n",
    "    callbacks=[\n",
    "        early_stopping(50),\n",
    "        log_evaluation(50)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e403b-daed-4500-96ec-6f8d10af36de",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739c9b7-471b-4fe9-8186-1ebb0156153c",
   "metadata": {},
   "source": [
    "#### Evaluating performance - trying different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa5187-6384-4a22-9dab-594dd858e30c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# trying out thresholds 0.3, 0.4 and 0.5 to try and find the optimum value\n",
    "for thresh in [0.3, 0.4, 0.5]:\n",
    "    y_pred_thresh = (y_pred_proba >= thresh).astype(int)\n",
    "    print(f\"\\nThreshold: {thresh}\")\n",
    "    print(confusion_matrix(y_test, y_pred_thresh))\n",
    "    print(classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "\n",
    "#print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "#print(\"\\nClassification Report:\")\n",
    "#print(classification_report(y_test, y_pred))\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80974b-7bd7-44d8-8241-22bb4929ae36",
   "metadata": {},
   "source": [
    "#### Evaluating performance - threshold value of 0.5 seems to be provide a balanced tradeoff between precision and recall. \n",
    "Only 34% of all predicted serious/fatal cases were actually serious/fatal so it's over-predicting a bit. The model was able to find 70% of all actual serious/fatal cases. Both Precision and Recall for Class 1 are better than random chance (0.34 vs 0.21 and 0.70 vs 0.5 respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f8d28-1523-418e-a2cc-2a869d7129fc",
   "metadata": {},
   "source": [
    "#### Evaluating performance - visualising Precision-Recall tradeoff as a function on Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6f7a8-97e9-4d82-8421-6f498f4078f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precision[:-1], label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba9694-d3ac-40c0-addb-b046d9f8bcf1",
   "metadata": {},
   "source": [
    "#### Evaluating performance \n",
    "- visualising ROC curve and Precision-Recall Curve. The model achieved ROC AUC of 0.725 which is good, but not great. Precision-Recall AUC was 0.417 which is not great but it is still almost double of what it would be by pure chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122836a-c25d-4988-b195-96bf55c80ecd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Predicted probabilities for the positive class\n",
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot both curves\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f\"AUC = {pr_auc:.3f}\", color=\"darkorange\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c2133-8131-4f52-964e-13678bf9612c",
   "metadata": {},
   "source": [
    "#### Determining feature importance \n",
    "- suprisingly, the number of vehicles feature was used over 300 times to split the data so it was \"the most contributing\" feature.\n",
    "- Also surprisingly - the road surface conditions grouping had the lowest contribution, although this may be due to it being correlated with another feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958562d-45e3-4ed5-9c58-f216b10823fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_imp = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "plt.title(\"Feature Importance (LightGBM)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa3c87-897a-4516-9e0f-42ddc14ee7f9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In conclusion, using a LGBM Classifier resulted in solid baseline.\n",
    "True Positive vs False Positive AUC ~ 0.725 → The model ranks cases well, above chance, but not strongly predictive.\n",
    "\n",
    "Precision-Recall AUC ~ 0.417 → Better than random (~0.21), but still indicates lots of false positives when trying to detect serious/fatal outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85134ff-a771-4270-99f1-23d5d3ae7826",
   "metadata": {},
   "source": [
    "## Potential Improvements\n",
    "Aspects that could be considered to improve the model:\n",
    "- Additional feature engineering could help - maybe some traffic and/or environmental contex.\n",
    "- Combined features - e.g. - driver age and casualty class.\n",
    "- Maybe trying different parameter values (threshold, # leaves, estimators)\n",
    "- Trying a different model - e.g. - Random Forest, CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b88d9-467a-41f0-a044-219c2f16a967",
   "metadata": {},
   "source": [
    "### Key Package Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eab544-84fa-4ce7-bd43-15d1e8f89c88",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "for dist in pkg_resources.working_set:\n",
    "    if dist.project_name.lower() in [\n",
    "        \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"lightgbm\", \"scikit-learn\"\n",
    "    ]:\n",
    "        print(f\"{dist.project_name}=={dist.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40df2d-56e0-4065-a845-bb0bcd35ce08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
